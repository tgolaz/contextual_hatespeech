{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T14:26:12.859797Z","iopub.status.busy":"2024-05-13T14:26:12.858940Z","iopub.status.idle":"2024-05-13T14:26:19.926633Z","shell.execute_reply":"2024-05-13T14:26:19.925650Z","shell.execute_reply.started":"2024-05-13T14:26:12.859760Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from transformers import BertTokenizer#, BertForSequenceClassification, AdamW\n","#from torch.utils.data import DataLoader, Dataset\n","#import torch\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T14:26:30.758708Z","iopub.status.busy":"2024-05-13T14:26:30.758315Z","iopub.status.idle":"2024-05-13T14:26:37.007835Z","shell.execute_reply":"2024-05-13T14:26:37.007061Z","shell.execute_reply.started":"2024-05-13T14:26:30.758677Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('./Sarcasm_on_Reddit/train-balanced-sarcasm.csv')[['comment', 'parent_comment', 'label']]\n","\n","#Data preprocessing to avoid further problems\n","df['comment'] = df['comment'].astype(str)\n","df['parent_comment'] = df['parent_comment'].astype(str)\n","df.dropna(inplace=True)\n","df['label'] = df['label'].astype(int)\n","df = df[df['label'].isin([0, 1])]\n","\n","#df['combined'] = list(zip(df['parent_comment'], df['comment']))\n","df['combined'] = df['parent_comment'] + \" [SEP] \" + df['comment']\n","\n","df_label_0 = df[df['label'] == 0].head(10000)\n","\n","# Filter the DataFrame for label = 1 and take the first 5000 entries\n","df_label_1 = df[df['label'] == 1].head(10000)\n","\n","# Concatenate the two DataFrames\n","df = pd.concat([df_label_0, df_label_1], ignore_index=True)\n","df = df.sample(frac=1, random_state=42).reset_index(drop=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T14:26:37.020757Z","iopub.status.busy":"2024-05-13T14:26:37.020407Z","iopub.status.idle":"2024-05-13T14:26:37.730292Z","shell.execute_reply":"2024-05-13T14:26:37.729307Z","shell.execute_reply.started":"2024-05-13T14:26:37.020727Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 231508/231508 [00:00<00:00, 787143.68B/s]\n"]}],"source":["#Tokenization\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Analyze the tokenized text to find [UNK] tokens\n","def analyze_tokenized_texts(texts):\n","    unk_count = 0\n","    total_count = 0\n","    for text in texts:\n","        tokens = tokenizer.tokenize(text)\n","        unk_count += tokens.count('[UNK]')\n","        total_count += len(tokens)\n","    return unk_count, total_count\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sarcastic UNK proportion: 8.887802601904211e-06\n","Non-sarcastic UNK proportion: 1.2791049676173258e-05\n"]}],"source":["# Analyze both sarcastic and non-sarcastic comments\n","sarcastic_texts = df[df['label'] == 1]['combined']\n","non_sarcastic_texts = df[df['label'] == 0]['combined']\n","\n","sarcastic_unk_count, sarcastic_total_count = analyze_tokenized_texts(sarcastic_texts)\n","non_sarcastic_unk_count, non_sarcastic_total_count = analyze_tokenized_texts(non_sarcastic_texts)\n","\n","# Calculate the proportion of [UNK] tokens to total tokens for both sarcastic and non-sarcastic comments\n","sarcastic_unk_proportion = sarcastic_unk_count / sarcastic_total_count\n","non_sarcastic_unk_proportion = non_sarcastic_unk_count / non_sarcastic_total_count\n","\n","print(f\"Sarcastic UNK proportion: {sarcastic_unk_proportion}\")\n","print(f\"Non-sarcastic UNK proportion: {non_sarcastic_unk_proportion}\")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sarcastic UNK comment proportion: 0.04%\n","Non-sarcastic UNK comment proportion: 0.05%\n"]}],"source":["def unk_comment_proportion(texts):\n","    unk_comment_count = 0\n","    total_comments = len(texts)\n","    for text in texts:\n","        tokens = tokenizer.tokenize(text)\n","        if '[UNK]' in tokens:\n","            unk_comment_count += 1\n","    return unk_comment_count / total_comments\n","\n","# Analyze both sarcastic and non-sarcastic comments\n","sarcastic_texts = df[df['label'] == 1]['combined']\n","non_sarcastic_texts = df[df['label'] == 0]['combined']\n","\n","sarcastic_unk_comment_proportion = unk_comment_proportion(sarcastic_texts)\n","non_sarcastic_unk_comment_proportion = unk_comment_proportion(non_sarcastic_texts)\n","\n","print(f\"Sarcastic UNK comment proportion: {sarcastic_unk_comment_proportion:.2%}\")\n","print(f\"Non-sarcastic UNK comment proportion: {non_sarcastic_unk_comment_proportion:.2%}\")"]},{"cell_type":"markdown","metadata":{},"source":["## "]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":1309,"sourceId":36545,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
