{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://luv-bansal.medium.com/fine-tuning-bert-for-text-classification-in-pytorch-503d97342db2 (code below)\n",
    "\n",
    "\n",
    "https://huggingface.co/transformers/v3.2.0/custom_datasets.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary #what is that, it works ?\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       The only thing I got from college is a caffein...\n",
      "1       I love it when professors draw a big question ...\n",
      "2       Remember the hundred emails from companies whe...\n",
      "3       Today my pop-pop told me I was not â€œforcedâ€ to...\n",
      "4       @VolphanCarol @littlewhitty @mysticalmanatee I...\n",
      "                              ...                        \n",
      "3463    The population spike in Chicago in 9 months is...\n",
      "3464    You'd think in the second to last English clas...\n",
      "3465    Iâ€™m finally surfacing after a holiday to Scotl...\n",
      "3466    Couldn't be prouder today. Well done to every ...\n",
      "3467    Overheard as my 13 year old games with a frien...\n",
      "Name: tweet, Length: 3468, dtype: object\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "3463    0\n",
      "3464    0\n",
      "3465    0\n",
      "3466    0\n",
      "3467    0\n",
      "Name: sarcastic, Length: 3468, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#import the dataset and create a trainig and testing and validation set\n",
    "\n",
    "\n",
    "isarcasm_data = pd.read_csv('isarcasm2022.csv')\n",
    "\n",
    "text = isarcasm_data.iloc[: , 1]\n",
    "isitsarcasm = isarcasm_data.iloc[: , 2]\n",
    "\n",
    "print(text)\n",
    "print(isitsarcasm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the class that represent the dataset\n",
    "\n",
    "class IsarcasmDataset(Dataset):\n",
    "   \n",
    "    def __init__(self, csv_file, root_dir, transform=None, train_size=0.7, test_size=0.15, val_size=0.15):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.rawisarcasmData = pd.read_csv(csv_file)\n",
    "        self.isarcasmData = self.rawisarcasmData.sample(frac=1).reset_index(drop=True) #randomly shuffle the set\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        train_data, temp_data = train_test_split(self.isarcasmData, test_size=test_size+val_size, random_state=42)\n",
    "        test_data, val_data = train_test_split(temp_data, test_size=val_size/(test_size+val_size), random_state=42)\n",
    "\n",
    "        self.train_set = train_data\n",
    "        self.test_set = test_data\n",
    "        self.validation_set = val_data\n",
    "\n",
    "        #self.save_sets()\n",
    "        \n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.isarcasmData)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        text = self.isarcasmData.iloc[idx , 1]\n",
    "        label = self.isarcasmData.iloc[idx , 2]\n",
    "        \n",
    "        sample = {'tweet': text, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def save_sets(self):\n",
    "       \n",
    "        # Save the datasets to separate CSV files\n",
    "        self.train_set.to_csv('train_dataset.csv', index=False)\n",
    "        self.test_set.to_csv('test_dataset.csv', index=False)\n",
    "        self.validation_set.to_csv('val_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3468\n",
      "      Unnamed: 0                                              tweet  \\\n",
      "350          809  Semester hasnâ€™t even started and Iâ€™ve already ...   \n",
      "430         2679  How long do you have be out of school before y...   \n",
      "3290        2608  Going out is overrated! On the couch watching ...   \n",
      "184         1618  Casually looking at the 06Z forecast it seems ...   \n",
      "2359        1773                        I passed my permit test!!!ðŸ¥µ   \n",
      "...          ...                                                ...   \n",
      "2411        2582     It b the memories we make everyday for me .. ðŸ’“   \n",
      "1812         502  Shaving in the shower without your glasses/con...   \n",
      "881          684  @DarrkIt @iamspade_ ya need a carry hit me up ...   \n",
      "3191        1251  Gods playing games today. Makin it 68 degrees ...   \n",
      "700         2506  https://t.co/ZOkVLoNM6Z\\r\\n#king810 @KING810FL...   \n",
      "\n",
      "      sarcastic                                           rephrase  sarcasm  \\\n",
      "350           1  I would say â€œthis isnâ€™t going to be a fun seme...      1.0   \n",
      "430           0                                                NaN      NaN   \n",
      "3290          0                                                NaN      NaN   \n",
      "184           0                                                NaN      NaN   \n",
      "2359          0                                                NaN      NaN   \n",
      "...         ...                                                ...      ...   \n",
      "2411          0                                                NaN      NaN   \n",
      "1812          1  Shaving your legs without your glasses/contact...      1.0   \n",
      "881           1                hit me up to play im an octane main      1.0   \n",
      "3191          0                                                NaN      NaN   \n",
      "700           0                                                NaN      NaN   \n",
      "\n",
      "      irony  satire  understatement  overstatement  rhetorical_question  \n",
      "350     0.0     0.0             0.0            0.0                  0.0  \n",
      "430     NaN     NaN             NaN            NaN                  NaN  \n",
      "3290    NaN     NaN             NaN            NaN                  NaN  \n",
      "184     NaN     NaN             NaN            NaN                  NaN  \n",
      "2359    NaN     NaN             NaN            NaN                  NaN  \n",
      "...     ...     ...             ...            ...                  ...  \n",
      "2411    NaN     NaN             NaN            NaN                  NaN  \n",
      "1812    0.0     0.0             0.0            0.0                  0.0  \n",
      "881     0.0     0.0             0.0            0.0                  0.0  \n",
      "3191    NaN     NaN             NaN            NaN                  NaN  \n",
      "700     NaN     NaN             NaN            NaN                  NaN  \n",
      "\n",
      "[520 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#instantiate the class\n",
    "\n",
    "Isarcasm_dataset = IsarcasmDataset(csv_file='isarcasm2022.csv',\n",
    "                                    root_dir='/')\n",
    "\n",
    "print(Isarcasm_dataset.__len__()) \n",
    "Isarcasm_dataset.__getitem__(4)\n",
    "\n",
    "\n",
    "#print(Isarcasm_dataset.test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader \n",
    "\n",
    "#dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
    "                        #shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, tokenizer,max_length):\n",
    "        super(BertDataset, self).__init__()\n",
    "        #self.root_dir=root_dir\n",
    "        self.train_csv=pd.read_csv('val_dataset.csv')\n",
    "        self.tokenizer=tokenizer\n",
    "        self.target=self.train_csv.iloc[:,2] #label in column 2\n",
    "        self.max_length=max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text1 = self.train_csv.iloc[index,1] #text im column 1\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus( \n",
    "            \n",
    "        #tokenizer settings https://stackoverflow.com/questions/61708486/whats-difference-between-tokenizer-encode-and-tokenizer-encode-plus-in-hugging\n",
    "        #https://huggingface.co/docs/transformers/en/main_classes/tokenizer   \n",
    "        #https://huggingface.co/docs/transformers/v4.40.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode_plus\n",
    "        \n",
    "            text1 , #sequnce to be encoded\n",
    "            None,  #text_pair ??\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True, #https://stackoverflow.com/questions/71679626/what-is-so-special-about-special-tokens\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        \n",
    "        # encodes the text using the BERT tokenizer's encode_plus method, which returns the input IDs, token type IDs, and attention mask.\n",
    "        ids = inputs[\"input_ids\"] #not really understood this part...\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'target': torch.tensor(self.train_csv.iloc[index, 2], dtype=torch.long) #label im column 2\n",
    "            }\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "\n",
    "'''\n",
    "#in an other project they use that\n",
    "DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "dataset= BertDataset(tokenizer, max_length=100) #instanciate the dataset, adapt\n",
    "dataloader=DataLoader(dataset=dataset,batch_size=32) #from, torch adapt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In line 4, we have initialized our pre-trained â€˜bert-base-uncasedâ€™ BERT model from Hugging face library and followed by initializing our linear dense layer for classifying movie reviews.\n",
    "\n",
    "Here, we use BCEWithLogitsLoss which combines a Sigmoid layer and the BCELoss in one single class because this version is more numerically stable than using a plain Sigmoid followed by a BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/docs/transformers/en/model_doc/bert\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.out = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self,ids,mask,token_type_ids):\n",
    "        _,o2= self.bert_model(ids,attention_mask=mask,token_type_ids=token_type_ids, return_dict=False)\n",
    "        \n",
    "        out= self.out(o2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "model=BERT()\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#Initialize Optimizer\n",
    "optimizer= optim.Adam(model.parameters(),lr= 0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "irst, we do not retrain our pre-trained BERT and train only the last linear dense layer.\n",
    "For this, we need to define it as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert_model.parameters():\n",
    "    param.requires_grad = False #fine tuning only not from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(epochs,dataloader,model,loss_fn,optimizer):\n",
    "    model.train()\n",
    "    for  epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        \n",
    "        loop=tqdm(enumerate(dataloader),leave=False,total=len(dataloader))\n",
    "        for batch, dl in loop:\n",
    "            ids=dl['ids']\n",
    "            token_type_ids=dl['token_type_ids']\n",
    "            mask= dl['mask']\n",
    "            label=dl['target']\n",
    "            label = label.unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output=model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "            label = label.type_as(output)\n",
    "\n",
    "            loss=loss_fn(output,label)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            pred = np.where(output >= 0, 1, 0)\n",
    "\n",
    "            num_correct = sum(1 for a, b in zip(pred, label) if a[0] == b[0])\n",
    "            num_samples = pred.shape[0]\n",
    "            accuracy = num_correct/num_samples\n",
    "            \n",
    "            print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "            \n",
    "            # Show progress while training\n",
    "            loop.set_description(f'Epoch={epoch}/{epochs}')\n",
    "            loop.set_postfix(loss=loss.item(),acc=accuracy)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/5:   6%|â–Œ         | 1/17 [00:04<01:09,  4.34s/it, acc=0.75, loss=0.645]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 24 / 32 with accuracy 75.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/5:  12%|â–ˆâ–        | 2/17 [00:09<01:09,  4.60s/it, acc=0.719, loss=0.645]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 23 / 32 with accuracy 71.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/5:  18%|â–ˆâ–Š        | 3/17 [00:13<01:02,  4.46s/it, acc=0.719, loss=0.632]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 23 / 32 with accuracy 71.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/5:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:17<00:56,  4.36s/it, acc=0.812, loss=0.604]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 26 / 32 with accuracy 81.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/5:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:22<00:52,  4.40s/it, acc=0.719, loss=0.626]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 23 / 32 with accuracy 71.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/5:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:27<00:52,  4.75s/it, acc=0.781, loss=0.598]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 25 / 32 with accuracy 78.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/5:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:33<00:50,  5.03s/it, acc=0.688, loss=0.638]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 22 / 32 with accuracy 68.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 27 / 32 with accuracy 84.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [57], line 7\u001b[0m, in \u001b[0;36mfinetune\u001b[1;34m(epochs, dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(epoch)\n\u001b[0;32m      6\u001b[0m loop\u001b[38;5;241m=\u001b[39mtqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader),leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader))\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, dl \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[0;32m      8\u001b[0m     ids\u001b[38;5;241m=\u001b[39mdl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mdl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Maxime\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Maxime\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Maxime\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Maxime\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Maxime\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn [54], line 17\u001b[0m, in \u001b[0;36mBertDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m     15\u001b[0m     text1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_csv\u001b[38;5;241m.\u001b[39miloc[index,\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m#text im column 1\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#tokenizer settings https://stackoverflow.com/questions/61708486/whats-difference-between-tokenizer-encode-and-tokenizer-encode-plus-in-hugging\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#https://huggingface.co/docs/transformers/en/main_classes/tokenizer   \u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#https://huggingface.co/docs/transformers/v4.40.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode_plus\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext1\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#sequnce to be encoded\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#text_pair ??\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#https://stackoverflow.com/questions/71679626/what-is-so-special-about-special-tokens\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# encodes the text using the BERT tokenizer's encode_plus method, which returns the input IDs, token type IDs, and attention mask.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m#not really understood this part...\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Maxime\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3037\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3028\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3029\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3030\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3034\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3035\u001b[0m )\n\u001b[1;32m-> 3037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[0;32m   3038\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   3039\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   3040\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3041\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3042\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3043\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3044\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3045\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3046\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3047\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3048\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3049\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3050\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3051\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3052\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3053\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3054\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3055\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3056\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Maxime\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    717\u001b[0m     )\n\u001b[1;32m--> 719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[0;32m    723\u001b[0m     first_ids,\n\u001b[0;32m    724\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    738\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    739\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Maxime\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils.py:705\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string or a list/tuple of strings when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `is_split_into_words=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "model=finetune(5, dataloader, model, loss_fn, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
